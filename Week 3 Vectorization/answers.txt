RECITATION
# WRITEUP 2
Use 32-bit aligned registers.


# WRITEUP 3
Seems like the ternary operator does not compare arrays element-wise.


# WRITEUP 4
The memory is not aligned. It should be possible to either reallign the 
b array by moving everything one place further or by making it possible
to align with an offset. This should definitely be faster.


# WRITEUP 5
The ffast-math flag was already set from a prior experiment! The addsd command 
is replaced by a addpd command.

clang -O3 example4.c -o example4; ./example4
The decimal floating point sum result is: 11.667578
The raw floating point sum result is: 0x1.755cccec10aa5p+3

clang -O3 -ffast-math  example4.c -o example4; ./example4
The decimal floating point sum result is: 11.667578
The raw floating point sum result is: 0x1.755cccec10aa3p+3


HOMEWORK
# WRITEUP 6
Without vectorisation:
Elapsed execution time: 0.051179 sec; N: 1024, I: 100000, __OP__: +, __TYPE__: uint32_t
With vectorisation:
Elapsed execution time: 0.014415 sec; N: 1024, I: 100000, __OP__: +, __TYPE__: uint32_t
With AVX instructions:
Elapsed execution time: 0.015936 sec; N: 1024, I: 100000, __OP__: +, __TYPE__: uint32_t

The last two timings are not that different, the mean just fluctuates a lot! We need 100 
times more data points!

Without vectorisation:
Elapsed execution time: 5.143872 sec; N: 1024, I: 10000000, __OP__: +, __TYPE__: uint32_t
With vectorisation:
Elapsed execution time: 1.236273 sec; N: 1024, I: 10000000, __OP__: +, __TYPE__: uint32_t
With AVX instructions:
Elapsed execution time: 1.240279 sec; N: 1024, I: 10000000, __OP__: +, __TYPE__: uint32_t

The speedup gained by vectorisation is about 4 times, for both vectorisations. This makes
sense, as the SSE vector registers are usually 128 bits wide, so 4 x 32 = 128.
That is only for the MacBook. When using the DTU cluster, we obtain the following.

Without vectorisation:
Elapsed execution time: 1.627987 sec; N: 1024, I: 10000000, __OP__: +, __TYPE__: uint32_t
With vectorisation:
Elapsed execution time: 1.623288 sec; N: 1024, I: 10000000, __OP__: +, __TYPE__: uint32_t
With AVX instructions:
Elapsed execution time: 0.955319 sec; N: 1024, I: 10000000, __OP__: +, __TYPE__: uint32_t

On the DTU cluster we cannot seem to disable vectorisation, but here the AVX2 vectorisation 
clearly yields an advantage, with a speedup of about 2. This is because the AVX registers
are 256 bits wide, instead of 128bits 


# WRITEUP 7
Without vectorisation there is a lot of pushing 'long' values around. With the initial 
vectorisation present we can see that the speedup is gained by the 'movdqa' command, 
which is fast because it uses an aligned move. 
